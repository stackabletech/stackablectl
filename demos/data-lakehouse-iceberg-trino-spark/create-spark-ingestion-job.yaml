# We can't simply create the SparkApplication object here as we have to wait for Kafka to be ready because
# * We currently don't restart failed Spark applications (see https://github.com/stackabletech/spark-k8s-operator/issues/157)
# * We currently auto-create topics and we need all the brokers to be available so that the topic is distributed among all the brokers
---
apiVersion: batch/v1
kind: Job
metadata:
  name: create-spark-ingestion-job
spec:
  template:
    spec:
      serviceAccountName: demo-serviceaccount
      initContainers:
        - name: wait-for-testdata
          image: docker.stackable.tech/stackable/tools:0.2.0-stackable0.3.0
          command: ["bash", "-c", "echo 'Waiting for all kafka brokers to be ready' && kubectl wait --for=condition=ready --timeout=30m pod -l app.kubernetes.io/instance=kafka -l app.kubernetes.io/name=kafka"]
      containers:
        - name: create-spark-ingestion-job
          image: docker.stackable.tech/stackable/tools:0.2.0-stackable0.3.0
          command: ["bash", "-c", "echo 'Submitting Spark job' && kubectl apply -f /tmp/manifest/spark-ingestion-job.yaml"]
          volumeMounts:
            - name: manifest
              mountPath: /tmp/manifest
      volumes:
      - name: manifest
        configMap:
          name: create-spark-ingestion-job-manifest
      restartPolicy: OnFailure
  backoffLimit: 50
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: create-spark-ingestion-job-manifest
data:
  spark-ingestion-job.yaml: |
    ---
    apiVersion: spark.stackable.tech/v1alpha1
    kind: SparkApplication
    metadata:
      name: spark-ingest-into-lakehouse
    spec:
      version: "1.0"
      sparkImage: docker.stackable.tech/demos/pyspark-k8s-with-kafka-and-iceberg:3.3.0-stackable0.2.0-1.1.0
      mode: cluster
      mainApplicationFile: local:///stackable/spark/jobs/spark-ingest-into-lakehouse.py
      # deps:
      #   packages:
      #     - org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.1.0
      #     - org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0
      sparkConf:
        spark.hadoop.fs.s3a.endpoint: http://minio:9000
        spark.hadoop.fs.s3a.path.style.access: "true"
        spark.hadoop.fs.s3a.access.key: trino
        spark.hadoop.fs.s3a.secret.key: trinotrino
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.lakehouse: org.apache.iceberg.spark.SparkCatalog
        spark.sql.catalog.lakehouse.type: hive
        spark.sql.catalog.lakehouse.uri: thrift://hive-iceberg:9083
        # Every merge into statements creates 8 files.
        # Paralleling is enough for the demo, might need to be increased (or omitted entirely) when merge larger data volumes
        spark.sql.shuffle.partitions: "8"
      volumes:
        - name: script
          configMap:
            name: write-iceberg-table-script
      job:
        resources:
          cpu:
            min: "100m"
            max: "1"
      driver:
        resources:
          cpu:
            min: "1"
            max: "1"
          memory:
            limit: "4Gi"
        volumeMounts:
          - name: script
            mountPath: /stackable/spark/jobs
      executor:
        instances: 4
        resources:
          cpu:
            min: "2"
            max: "4"
          memory:
            limit: "8Gi"
        volumeMounts:
          - name: script
            mountPath: /stackable/spark/jobs
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: write-iceberg-table-script
data:
  spark-ingest-into-lakehouse.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, LongType, ShortType, FloatType, DoubleType, BooleanType, TimestampType, MapType, ArrayType
    from pyspark.sql.functions import col, from_json, expr
    import time
    from datetime import datetime, timedelta

    spark = SparkSession.builder.appName("spark-ingest-into-lakehouse").getOrCreate()
    # spark.sparkContext.setLogLevel("DEBUG")

    spark.sql("CREATE SCHEMA IF NOT EXISTS lakehouse.water_levels LOCATION 's3a://lakehouse/water-levels/'")
    spark.sql("CREATE SCHEMA IF NOT EXISTS lakehouse.smart_city LOCATION 's3a://lakehouse/smart-city/'")
    spark.sql("CREATE SCHEMA IF NOT EXISTS lakehouse.github LOCATION 's3a://lakehouse/github/'")

    # TODO add PARTITIONED BY (days(timestamp))
    # Currently fails with org.apache.spark.sql.AnalysisException: days(timestamp) ASC NULLS FIRST is not currently supported
    # Don't forget to add option("fanout-enabled", "true") to iceberg sink as well
    # see https://github.com/apache/iceberg/issues/5625

    # TODO add Iceberg option 'write.object-storage.enabled' = true
    # Currently drop tables in Trino won't remove all the data files in that case...

    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.water_levels.measurements (station_uuid string, timestamp timestamp, value float) USING iceberg PARTITIONED BY (days(timestamp)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.water_levels.stations (station_uuid string, number bigint, short_name string, long_name string, km float, agency string, latitude double, longitude double, water_short_name string, water_long_name string) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_bike_status (bike_id string, vehicle_type_id string, latitude double, longitude double, is_reserved boolean, is_disabled boolean, last_reported timestamp) USING iceberg PARTITIONED BY (days(last_reported)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_station_information (station_id string, name string, latitude double, longitude double) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_station_status (station_id string, num_bikes_available short, is_installed boolean, is_renting boolean, is_returning boolean, last_reported timestamp) USING iceberg PARTITIONED BY (days(last_reported)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    # Column list is generated by calling print(schema.simpleString().replace(':', ' ').replace(',', ', '))
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.github.repos (id bigint, node_id string, name string, full_name string, private boolean, owner struct<login string, id bigint, node_id string, avatar_url string, gravatar_id string, url string, html_url string, followers_url string, following_url string, gists_url string, starred_url string, subscriptions_url string, organizations_url string, repos_url string, events_url string, received_events_url string, type string, site_admin boolean>, html_url string, description string, fork boolean, url string, forks_url string, keys_url string, collaborators_url string, teams_url string, hooks_url string, issue_events_url string, events_url string, assignees_url string, branches_url string, tags_url string, blobs_url string, git_tags_url string, git_refs_url string, trees_url string, statuses_url string, languages_url string, stargazers_url string, contributors_url string, subscribers_url string, subscription_url string, commits_url string, git_commits_url string, comments_url string, issue_comment_url string, contents_url string, compare_url string, merges_url string, archive_url string, downloads_url string, issues_url string, pulls_url string, milestones_url string, notifications_url string, labels_url string, releases_url string, deployments_url string, created_at timestamp, updated_at timestamp, pushed_at timestamp, git_url string, ssh_url string, clone_url string, svn_url string, homepage string, size bigint, stargazers_count bigint, watchers_count bigint, language string, has_issues boolean, has_projects boolean, has_downloads boolean, has_wiki boolean, has_pages boolean, has_discussions boolean, forks_count bigint, mirror_url string, archived boolean, disabled string, open_issues_count bigint, license struct<key string, name string, spdx_id string, url string, node_id string>, allow_forking boolean, is_template boolean, web_commit_signoff_required boolean, topics array<string>, visibility string, forks bigint, open_issues bigint, watchers bigint, default_branch string, permissions struct<admin boolean, maintain boolean, push boolean, triage boolean, pull boolean>) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.github.commits (sha string, node_id string, commit struct<author struct<name string, email string, date timestamp>, committer struct<name string, email string, date timestamp>, message string, tree struct<sha string, url string>, url string, comment_count bigint, verification struct<verified boolean, reason string, signature string, payload string>>, url string, html_url string, comments_url string, author struct<login string, id bigint, node_id string, avatar_url string, gravatar_id string, url string, html_url string, followers_url string, following_url string, gists_url string, starred_url string, subscriptions_url string, organizations_url string, repos_url string, events_url string, received_events_url string, type string, site_admin boolean>, committer struct<login string, id bigint, node_id string, avatar_url string, gravatar_id string, url string, html_url string, followers_url string, following_url string, gists_url string, starred_url string, subscriptions_url string, organizations_url string, repos_url string, events_url string, received_events_url string, type string, site_admin boolean>, parents struct<sha string, url string, html_url string>) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")







    def upsertWaterLevelsStationInformation(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("waterLevelsStationInformationUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.water_levels.stations as t
        USING
          (
            SELECT station_uuid, number, short_name, long_name, km, agency, latitude, longitude, water_short_name, water_long_name
            FROM waterLevelsStationInformationUpserts
            WHERE (station_uuid, kafka_timestamp) IN (SELECT station_uuid, max(kafka_timestamp) FROM waterLevelsStationInformationUpserts GROUP BY station_uuid)
          ) as u
        ON u.station_uuid = t.station_uuid
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("uuid", StringType(), True), \
        StructField("number", StringType(), True),  \
        StructField("shortname", StringType(), True), \
        StructField("longname", StringType(), True), \
        StructField("km", FloatType(), True), \
        StructField("agency", StringType(), True), \
        StructField("latitude", DoubleType(), True), \
        StructField("longitude", DoubleType(), True), \
        StructField("water", \
            StructType([StructField("shortname", StringType(), True), StructField("longname", StringType(), True)]), \
        True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "water_levels_stations") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr( \
        "json.uuid as station_uuid", \
        "cast(json.number as bigint) as number", \
        "json.shortname as short_name", \
        "json.longname as long_name", \
        "json.km", \
        "json.agency", \
        "json.latitude", \
        "json.longitude", \
        "json.water.shortname as water_short_name", \
        "json.water.longname as water_long_name", \
        "timestamp as kafka_timestamp" \
    ) \
    .writeStream \
    .queryName("ingest water_level stations") \
    .format("iceberg") \
    .foreachBatch(upsertWaterLevelsStationInformation) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/water-levels/checkpoints/stations") \
    .start()





    def upsertWaterLevelsMeasurements(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("waterLevelsMeasurementsUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.water_levels.measurements as t
        USING (SELECT DISTINCT * FROM waterLevelsMeasurementsUpserts) as u
        ON u.station_uuid = t.station_uuid AND u.timestamp = t.timestamp
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_uuid", StringType(), True), \
        StructField("timestamp", TimestampType(), True), \
        StructField("value", FloatType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "water_levels_measurements") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 50000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .select("json.station_uuid", "json.timestamp", "json.value") \
    .writeStream \
    .queryName("ingest water_level measurements") \
    .format("iceberg") \
    .foreachBatch(upsertWaterLevelsMeasurements) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/water-levels/checkpoints/measurements") \
    .start()








    def upsertSharedBikesStationInformation(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesStationInformationUpserts")

      # We have to remove all station_id with less than 5 characters as they contain duplicates
      # This causes org.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.
      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_station_information as t
        USING
          (
            SELECT station_id, name, latitude, longitude
            FROM sharedBikesStationInformationUpserts
            WHERE LENGTH(station_id) >= 5
              AND (station_id, kafka_timestamp) IN (SELECT station_id, max(kafka_timestamp) FROM sharedBikesStationInformationUpserts GROUP BY station_id)
          ) as u
        ON u.station_id = t.station_id
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_id", StringType(), True), \
        StructField("lat", DoubleType(), True), \
        StructField("lon", DoubleType(), True), \
        StructField("name", StringType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "shared_bikes_station_information") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.station_id", "json.name as name", "json.lat as latitude", "json.lon as longitude", "timestamp as kafka_timestamp") \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_station_information") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesStationInformation) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_station_information") \
    .start()






    def upsertSharedBikesStationStatus(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesStationStatusUpserts")

      # Upsert into the shared_bikes_station_status table, only inserting records if they don't already exist.
      # In case they don't exist, we only insert the new records instead of replacing the old to keep the history,
      # thus enabling to build reports on the usage change over time
      #
      # We have to remove all station_id with less than 5 characters as they contain duplicates
      # This causes org.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.
      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_station_status as t
        USING (SELECT DISTINCT station_id, num_bikes_available, is_installed, is_renting, is_returning, last_reported FROM sharedBikesStationStatusUpserts WHERE LENGTH(station_id) >= 5) as u
        ON u.station_id = t.station_id AND u.last_reported = t.last_reported
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_id", StringType(), True), \
        StructField("is_installed", BooleanType(), True), \
        StructField("last_reported", TimestampType(), True), \
        StructField("num_bikes_available", ShortType(), True), \
        StructField("is_renting", BooleanType(), True), \
        StructField("is_returning", BooleanType(), True), \
        StructField("vehicle_types_available", ArrayType(StructType([StructField("count", ShortType(), True), StructField("vehicle_type_id", StringType(), True)]), True), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "shared_bikes_station_status") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr( \
        "json.station_id", \
        "json.num_bikes_available", \
        "json.is_installed", \
        "json.is_renting", \
        "json.is_returning", \
        "json.last_reported" \
    ) \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_station_status") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesStationStatus) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_station_status") \
    .start()






    def upsertSharedBikesBikeStatus(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesBikeStatusUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_bike_status as t
        USING (SELECT DISTINCT bike_id, vehicle_type_id, latitude, longitude, is_reserved, is_disabled, last_reported FROM sharedBikesBikeStatusUpserts) as u
        ON u.bike_id = t.bike_id AND u.last_reported = t.last_reported
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("lat", DoubleType(), True), \
        StructField("lon", DoubleType(), True), \
        StructField("bike_id", StringType(), True), \
        StructField("is_reserved", BooleanType(), True), \
        StructField("is_disabled", BooleanType(), True), \
        StructField("vehicle_type_id", StringType(), True), \
        StructField("last_reported", TimestampType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "shared_bikes_bike_status") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.bike_id", "json.vehicle_type_id", "json.lat as latitude", "json.lon as longitude", "json.is_reserved", "json.is_disabled", "json.last_reported") \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_bike_status") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesBikeStatus) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_bike_status") \
    .start()

    def upsertGithubRepos(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("githubReposUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.github.repos as t
        USING
          (
            SELECT *
            FROM githubReposUpserts
            WHERE (id, kafka_timestamp) IN (SELECT id, max(kafka_timestamp) FROM githubReposUpserts GROUP BY id)
          ) as u
        ON u.id = t.id
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
      StructField("id", LongType(), True), \
      StructField("node_id", StringType(), True), \
      StructField("name", StringType(), True), \
      StructField("full_name", StringType(), True), \
      StructField("private", BooleanType(), True), \
      StructField("owner", StructType([ \
        StructField("login", StringType(), True), \
        StructField("id", LongType(), True), \
        StructField("node_id", StringType(), True), \
        StructField("avatar_url", StringType(), True), \
        StructField("gravatar_id", StringType(), True), \
        StructField("url", StringType(), True), \
        StructField("html_url", StringType(), True), \
        StructField("followers_url", StringType(), True), \
        StructField("following_url", StringType(), True), \
        StructField("gists_url", StringType(), True), \
        StructField("starred_url", StringType(), True), \
        StructField("subscriptions_url", StringType(), True), \
        StructField("organizations_url", StringType(), True), \
        StructField("repos_url", StringType(), True), \
        StructField("events_url", StringType(), True), \
        StructField("received_events_url", StringType(), True), \
        StructField("type", StringType(), True), \
        StructField("site_admin", BooleanType(), True), \
      ]), True), \
      StructField("html_url", StringType(), True), \
      StructField("description", StringType(), True), \
      StructField("fork", BooleanType(), True), \
      StructField("url", StringType(), True), \
      StructField("forks_url", StringType(), True), \
      StructField("keys_url", StringType(), True), \
      StructField("collaborators_url", StringType(), True), \
      StructField("teams_url", StringType(), True), \
      StructField("hooks_url", StringType(), True), \
      StructField("issue_events_url", StringType(), True), \
      StructField("events_url", StringType(), True), \
      StructField("assignees_url", StringType(), True), \
      StructField("branches_url", StringType(), True), \
      StructField("tags_url", StringType(), True), \
      StructField("blobs_url", StringType(), True), \
      StructField("git_tags_url", StringType(), True), \
      StructField("git_refs_url", StringType(), True), \
      StructField("trees_url", StringType(), True), \
      StructField("statuses_url", StringType(), True), \
      StructField("languages_url", StringType(), True), \
      StructField("stargazers_url", StringType(), True), \
      StructField("contributors_url", StringType(), True), \
      StructField("subscribers_url", StringType(), True), \
      StructField("subscription_url", StringType(), True), \
      StructField("commits_url", StringType(), True), \
      StructField("git_commits_url", StringType(), True), \
      StructField("comments_url", StringType(), True), \
      StructField("issue_comment_url", StringType(), True), \
      StructField("contents_url", StringType(), True), \
      StructField("compare_url", StringType(), True), \
      StructField("merges_url", StringType(), True), \
      StructField("archive_url", StringType(), True), \
      StructField("downloads_url", StringType(), True), \
      StructField("issues_url", StringType(), True), \
      StructField("pulls_url", StringType(), True), \
      StructField("milestones_url", StringType(), True), \
      StructField("notifications_url", StringType(), True), \
      StructField("labels_url", StringType(), True), \
      StructField("releases_url", StringType(), True), \
      StructField("deployments_url", StringType(), True), \
      StructField("created_at", TimestampType(), True), \
      StructField("updated_at", TimestampType(), True), \
      StructField("pushed_at", TimestampType(), True), \
      StructField("git_url", StringType(), True), \
      StructField("ssh_url", StringType(), True), \
      StructField("clone_url", StringType(), True), \
      StructField("svn_url", StringType(), True), \
      StructField("homepage", StringType(), True), \
      StructField("size", LongType(), True), \
      StructField("stargazers_count", LongType(), True), \
      StructField("watchers_count", LongType(), True), \
      StructField("language", StringType(), True), \
      StructField("has_issues", BooleanType(), True), \
      StructField("has_projects", BooleanType(), True), \
      StructField("has_downloads", BooleanType(), True), \
      StructField("has_wiki", BooleanType(), True), \
      StructField("has_pages", BooleanType(), True), \
      StructField("has_discussions", BooleanType(), True), \
      StructField("forks_count", LongType(), True), \
      StructField("mirror_url", StringType(), True), \
      StructField("archived", BooleanType(), True), \
      StructField("disabled", StringType(), True), \
      StructField("open_issues_count", LongType(), True), \
      StructField("license", StructType([ \
        StructField("key", StringType(), True), \
        StructField("name", StringType(), True), \
        StructField("spdx_id", StringType(), True), \
        StructField("url", StringType(), True), \
        StructField("node_id", StringType(), True), \
      ]), True), \
      StructField("allow_forking", BooleanType(), True), \
      StructField("is_template", BooleanType(), True), \
      StructField("web_commit_signoff_required", BooleanType(), True), \
      StructField("topics", ArrayType(StringType(), True), True), \
      StructField("visibility", StringType(), True), \
      StructField("forks", LongType(), True), \
      StructField("open_issues", LongType(), True), \
      StructField("watchers", LongType(), True), \
      StructField("default_branch", StringType(), True), \
      StructField("permissions", StructType([ \
        StructField("admin", BooleanType(), True), \
        StructField("maintain", BooleanType(), True), \
        StructField("push", BooleanType(), True), \
        StructField("triage", BooleanType(), True), \
        StructField("pull", BooleanType(), True), \
      ]), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "github_repos") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.*", "timestamp as kafka_timestamp") \
    .writeStream \
    .queryName("ingest github repos") \
    .format("iceberg") \
    .foreachBatch(upsertGithubRepos) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/github/checkpoints/repos") \
    .start()

    def upsertGithubCommits(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("githubCommitsUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.github.commits as t
        USING
          (
            SELECT *
            FROM githubCommitsUpserts
            WHERE (sha, kafka_timestamp) IN (SELECT sha, max(kafka_timestamp) FROM githubCommitsUpserts GROUP BY sha)
          ) as u
        ON u.sha = t.sha
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
      StructField("sha", StringType(), True), \
      StructField("node_id", StringType(), True), \
      StructField("name", StringType(), True), \
      StructField("full_name", StringType(), True), \
      StructField("private", BooleanType(), True), \
      StructField("commit", StructType([ \
        StructField("author", StructType([StructField("name", StringType(), True), StructField("email", StringType(), True), StructField("date", TimestampType(), True)]), True), \
        StructField("committer", StructType([StructField("name", StringType(), True), StructField("email", StringType(), True), StructField("date", TimestampType(), True)]), True), \
        StructField("message", StringType(), True), \
        StructField("tree", StructType([StructField("sha", StringType(), True), StructField("url", StringType(), True)]), True), \
        StructField("url", StringType(), True), \
        StructField("comment_count", LongType(), True), \
        StructField("verification", StructType([StructField("verified", BooleanType(), True), StructField("reason", StringType(), True), StructField("signature", StringType(), True), StructField("payload", StringType(), True)]), True), \
      ]), True), \
      StructField("url", StringType(), True), \
      StructField("html_url", StringType(), True), \
      StructField("comments_url", StringType(), True), \
      StructField("author", StructType([ \
        StructField("login", StringType(), True), \
        StructField("id", LongType(), True), \
        StructField("node_id", StringType(), True), \
        StructField("avatar_url", StringType(), True), \
        StructField("gravatar_id", StringType(), True), \
        StructField("url", StringType(), True), \
        StructField("html_url", StringType(), True), \
        StructField("followers_url", StringType(), True), \
        StructField("following_url", StringType(), True), \
        StructField("gists_url", StringType(), True), \
        StructField("starred_url", StringType(), True), \
        StructField("subscriptions_url", StringType(), True), \
        StructField("organizations_url", StringType(), True), \
        StructField("repos_url", StringType(), True), \
        StructField("events_url", StringType(), True), \
        StructField("received_events_url", StringType(), True), \
        StructField("type", StringType(), True), \
        StructField("site_admin", BooleanType(), True), \
      ]), True), \
      StructField("committer", StructType([ \
        StructField("login", StringType(), True), \
        StructField("id", LongType(), True), \
        StructField("node_id", StringType(), True), \
        StructField("avatar_url", StringType(), True), \
        StructField("gravatar_id", StringType(), True), \
        StructField("url", StringType(), True), \
        StructField("html_url", StringType(), True), \
        StructField("followers_url", StringType(), True), \
        StructField("following_url", StringType(), True), \
        StructField("gists_url", StringType(), True), \
        StructField("starred_url", StringType(), True), \
        StructField("subscriptions_url", StringType(), True), \
        StructField("organizations_url", StringType(), True), \
        StructField("repos_url", StringType(), True), \
        StructField("events_url", StringType(), True), \
        StructField("received_events_url", StringType(), True), \
        StructField("type", StringType(), True), \
        StructField("site_admin", BooleanType(), True), \
      ]), True), \
      StructField("parents", StructType([ \
        StructField("sha", StringType(), True), \
        StructField("url", StringType(), True), \
        StructField("html_url", StringType(), True), \
      ]), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "github_commits") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.*", "timestamp as kafka_timestamp") \
    .writeStream \
    .queryName("ingest github commits") \
    .format("iceberg") \
    .foreachBatch(upsertGithubCommits) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/github/checkpoints/commits") \
    .start()








    # key: table name
    # value: compaction strategy
    tables_to_compact = {
        "lakehouse.water_levels.stations": "",
        "lakehouse.water_levels.measurements": ", strategy => 'sort', sort_order => 'timestamp DESC NULLS LAST,station_uuid ASC NULLS LAST'",
        "lakehouse.smart_city.shared_bikes_station_information": "",
        "lakehouse.smart_city.shared_bikes_station_status": ", strategy => 'sort', sort_order => 'last_reported DESC NULLS LAST,station_id ASC NULLS LAST'",
        "lakehouse.smart_city.shared_bikes_bike_status": "",
        "lakehouse.github.repos": "",
        "lakehouse.github.commits": "",
    }

    while True:
        expire_before = (datetime.now() - timedelta(hours=12)).strftime("%Y-%m-%d %H:%M:%S")
        for table, table_compaction_strategy in tables_to_compact.items():
            print(f"[{table}] Expiring snapshots older than 12 hours ({expire_before})")
            spark.sql(f"CALL lakehouse.system.expire_snapshots(table => '{table}', older_than => TIMESTAMP '{expire_before}', retain_last => 50, stream_results => true)")

            print(f"[{table}] Removing orphaned files")
            spark.sql(f"CALL lakehouse.system.remove_orphan_files(table => '{table}')")

            print(f"[{table}] Starting compaction")
            spark.sql(f"CALL lakehouse.system.rewrite_data_files(table => '{table}'{table_compaction_strategy})")
            print(f"[{table}] Finished compaction")

        print("All tables compacted. Waiting 25min before scheduling next run...")
        time.sleep(25 * 60) # Assuming compaction takes 5 min run every 30 minutes
