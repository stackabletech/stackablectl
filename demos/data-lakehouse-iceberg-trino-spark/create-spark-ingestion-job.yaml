# We can't simply create the SparkApplication object here as we have to wait for Kafka to be ready because
# * We currently don't restart failed Spark applications (see https://github.com/stackabletech/spark-k8s-operator/issues/157)
# * We currently auto-create topics and we need all the brokers to be available so that the topic is distributed among all the brokers
---
apiVersion: batch/v1
kind: Job
metadata:
  name: create-spark-ingestion-job
spec:
  template:
    spec:
      serviceAccountName: demo-serviceaccount
      initContainers:
        - name: wait-for-kafka
          image: docker.stackable.tech/stackable/tools:1.0.0-stackable23.11.0
          command: ["bash", "-c", "echo 'Waiting for all kafka brokers to be ready' && kubectl wait --for=condition=ready --timeout=30m pod -l app.kubernetes.io/instance=kafka -l app.kubernetes.io/name=kafka"]
      containers:
        - name: create-spark-ingestion-job
          image: docker.stackable.tech/stackable/tools:1.0.0-stackable23.11.0
          command: ["bash", "-c", "echo 'Submitting Spark job' && kubectl apply -f /tmp/manifest/spark-ingestion-job.yaml"]
          volumeMounts:
            - name: manifest
              mountPath: /tmp/manifest
      volumes:
        - name: manifest
          configMap:
            name: create-spark-ingestion-job-manifest
      restartPolicy: OnFailure
  backoffLimit: 50
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: create-spark-ingestion-job-manifest
data:
  spark-ingestion-job.yaml: |
    ---
    apiVersion: spark.stackable.tech/v1alpha1
    kind: SparkApplication
    metadata:
      name: spark-ingest-into-lakehouse
    spec:
      version: "1.0"
      sparkImage:
        custom: docker.stackable.tech/demos/pyspark-k8s-with-kafka-and-iceberg:3.3.0-stackable0.2.0
        productVersion: 3.3.0
      mode: cluster
      mainApplicationFile: local:///stackable/spark/jobs/spark-ingest-into-lakehouse.py
      # deps:
      #   packages:
      #     - org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.1.0
      #     - org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0
      s3connection:
        reference: minio
      sparkConf:
        spark.kubernetes.container.image.pullPolicy: Always
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.catalog.lakehouse: org.apache.iceberg.spark.SparkCatalog
        spark.sql.catalog.lakehouse.type: hive
        spark.sql.catalog.lakehouse.uri: thrift://hive-iceberg:9083
        # Every merge into statements creates 8 files.
        # Paralleling is enough for the demo, might need to be increased (or omitted entirely) when merge larger data volumes
        spark.sql.shuffle.partitions: "8"
    
        # As of 2023-10-31 the operator does not set this
        spark.executor.cores: "4"
      volumes:
        - name: script
          configMap:
            name: write-iceberg-table-script
        - name: tls
          ephemeral:
            volumeClaimTemplate:
              metadata:
                annotations:
                  secrets.stackable.tech/class: tls
                  secrets.stackable.tech/scope: pod
                  secrets.stackable.tech/format: tls-pkcs12
                  secrets.stackable.tech/format.compatibility.tls-pkcs12.password: changeit
                  secrets.stackable.tech/backend.autotls.cert.lifetime: 14d # To prevent frequent restarts
              spec:
                accessModes:
                  - ReadWriteOnce
                resources:
                  requests:
                    storage: "1"
                storageClassName: secrets.stackable.tech
      job:
        config:
          resources:
            cpu:
              min: "100m"
              max: "1"
      driver:
        config:
          resources:
            cpu:
              min: "1"
              max: "1"
            memory:
              limit: "8Gi"
          volumeMounts:
            - name: script
              mountPath: /stackable/spark/jobs
            - name: tls
              mountPath: /stackable/tls
      executor:
        replicas: 4
        config:
          resources:
            cpu:
              min: "2"
              max: "4"
            memory:
              limit: "8Gi"
          volumeMounts:
            - name: script
              mountPath: /stackable/spark/jobs
            - name: tls
              mountPath: /stackable/tls
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: write-iceberg-table-script
data:
  spark-ingest-into-lakehouse.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.types import StructType, StructField, StringType, LongType, ShortType, FloatType, DoubleType, BooleanType, TimestampType, MapType, ArrayType
    from pyspark.sql.functions import col, from_json, expr
    from datetime import datetime, timedelta
    import subprocess
    import time

    spark = SparkSession.builder.appName("spark-ingest-into-lakehouse").getOrCreate()
    # spark.sparkContext.setLogLevel("DEBUG")

    spark.sql("CREATE SCHEMA IF NOT EXISTS lakehouse.water_levels LOCATION 's3a://lakehouse/water-levels/'")
    spark.sql("CREATE SCHEMA IF NOT EXISTS lakehouse.smart_city LOCATION 's3a://lakehouse/smart-city/'")

    # TODO add PARTITIONED BY (days(timestamp))
    # Currently fails with org.apache.spark.sql.AnalysisException: days(timestamp) ASC NULLS FIRST is not currently supported
    # Don't forget to add option("fanout-enabled", "true") to iceberg sink as well
    # see https://github.com/apache/iceberg/issues/5625

    # TODO add Iceberg option 'write.object-storage.enabled' = true
    # Currently drop tables in Trino won't remove all the data files in that case...

    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.water_levels.measurements (station_uuid string, timestamp timestamp, value float) USING iceberg PARTITIONED BY (days(timestamp)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.water_levels.stations (station_uuid string, number bigint, short_name string, long_name string, km float, agency string, latitude double, longitude double, water_short_name string, water_long_name string) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_bike_status (bike_id string, vehicle_type_id string, latitude double, longitude double, is_reserved boolean, is_disabled boolean, last_reported timestamp) USING iceberg PARTITIONED BY (days(last_reported)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_station_information (station_id string, name string, latitude double, longitude double) USING iceberg TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")
    spark.sql("CREATE TABLE IF NOT EXISTS lakehouse.smart_city.shared_bikes_station_status (station_id string, num_bikes_available short, is_installed boolean, is_renting boolean, is_returning boolean, last_reported timestamp) USING iceberg PARTITIONED BY (days(last_reported)) TBLPROPERTIES ('format-version' = 2, format = 'PARQUET')")

    kafkaOptions = {
      "kafka.bootstrap.servers": "kafka-broker-default.default.svc.cluster.local:9093",
      "kafka.security.protocol": "SSL",
      "kafka.ssl.truststore.location": "/stackable/tls/truststore.p12",
      "kafka.ssl.truststore.password": "changeit",
      "kafka.ssl.truststore.type": "PKCS12",
      "kafka.ssl.keystore.location": "/stackable/tls/keystore.p12",
      "kafka.ssl.keystore.password": "changeit",
      "kafka.ssl.keystore.type": "PKCS12",
    }

    def upsertWaterLevelsStationInformation(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("waterLevelsStationInformationUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.water_levels.stations as t
        USING
          (
            SELECT station_uuid, number, short_name, long_name, km, agency, latitude, longitude, water_short_name, water_long_name
            FROM waterLevelsStationInformationUpserts
            WHERE (station_uuid, kafka_timestamp) IN (SELECT station_uuid, max(kafka_timestamp) FROM waterLevelsStationInformationUpserts GROUP BY station_uuid)
          ) as u
        ON u.station_uuid = t.station_uuid
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("uuid", StringType(), True), \
        StructField("number", StringType(), True),  \
        StructField("shortname", StringType(), True), \
        StructField("longname", StringType(), True), \
        StructField("km", FloatType(), True), \
        StructField("agency", StringType(), True), \
        StructField("latitude", DoubleType(), True), \
        StructField("longitude", DoubleType(), True), \
        StructField("water", \
            StructType([StructField("shortname", StringType(), True), StructField("longname", StringType(), True)]), \
        True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .options(**kafkaOptions) \
    .option("subscribe", "water_levels_stations") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr( \
        "json.uuid as station_uuid", \
        "cast(json.number as bigint) as number", \
        "json.shortname as short_name", \
        "json.longname as long_name", \
        "json.km", \
        "json.agency", \
        "json.latitude", \
        "json.longitude", \
        "json.water.shortname as water_short_name", \
        "json.water.longname as water_long_name", \
        "timestamp as kafka_timestamp" \
    ) \
    .writeStream \
    .queryName("ingest water_level stations") \
    .format("iceberg") \
    .foreachBatch(upsertWaterLevelsStationInformation) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/water-levels/checkpoints/stations") \
    .start()


    def upsertWaterLevelsMeasurements(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("waterLevelsMeasurementsUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.water_levels.measurements as t
        USING (SELECT DISTINCT * FROM waterLevelsMeasurementsUpserts) as u
        ON u.station_uuid = t.station_uuid AND u.timestamp = t.timestamp
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_uuid", StringType(), True), \
        StructField("timestamp", TimestampType(), True), \
        StructField("value", FloatType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .options(**kafkaOptions) \
    .option("subscribe", "water_levels_measurements") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 50000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .select("json.station_uuid", "json.timestamp", "json.value") \
    .writeStream \
    .queryName("ingest water_level measurements") \
    .format("iceberg") \
    .foreachBatch(upsertWaterLevelsMeasurements) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/water-levels/checkpoints/measurements") \
    .start()


    def upsertSharedBikesStationInformation(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesStationInformationUpserts")

      # We have to remove all station_id with less than 5 characters as they contain duplicates
      # This causes org.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.
      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_station_information as t
        USING
          (
            SELECT station_id, name, latitude, longitude
            FROM sharedBikesStationInformationUpserts
            WHERE LENGTH(station_id) >= 5
              AND (station_id, kafka_timestamp) IN (SELECT station_id, max(kafka_timestamp) FROM sharedBikesStationInformationUpserts GROUP BY station_id)
          ) as u
        ON u.station_id = t.station_id
        WHEN MATCHED THEN UPDATE SET *
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_id", StringType(), True), \
        StructField("lat", DoubleType(), True), \
        StructField("lon", DoubleType(), True), \
        StructField("name", StringType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .options(**kafkaOptions) \
    .option("subscribe", "shared_bikes_station_information") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)", "timestamp") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.station_id", "json.name as name", "json.lat as latitude", "json.lon as longitude", "timestamp as kafka_timestamp") \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_station_information") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesStationInformation) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_station_information") \
    .start()


    def upsertSharedBikesStationStatus(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesStationStatusUpserts")

      # Upsert into the shared_bikes_station_status table, only inserting records if they don't already exist.
      # In case they don't exist, we only insert the new records instead of replacing the old to keep the history,
      # thus enabling to build reports on the usage change over time
      #
      # We have to remove all station_id with less than 5 characters as they contain duplicates
      # This causes org.apache.spark.SparkException: The ON search condition of the MERGE statement matched a single row from the target table with multiple rows of the source table. This could result in the target row being operated on more than once with an update or delete operation and is not allowed.
      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_station_status as t
        USING (SELECT DISTINCT station_id, num_bikes_available, is_installed, is_renting, is_returning, last_reported FROM sharedBikesStationStatusUpserts WHERE LENGTH(station_id) >= 5) as u
        ON u.station_id = t.station_id AND u.last_reported = t.last_reported
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("station_id", StringType(), True), \
        StructField("is_installed", BooleanType(), True), \
        StructField("last_reported", TimestampType(), True), \
        StructField("num_bikes_available", ShortType(), True), \
        StructField("is_renting", BooleanType(), True), \
        StructField("is_returning", BooleanType(), True), \
        StructField("vehicle_types_available", ArrayType(StructType([StructField("count", ShortType(), True), StructField("vehicle_type_id", StringType(), True)]), True), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .options(**kafkaOptions) \
    .option("subscribe", "shared_bikes_station_status") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr( \
        "json.station_id", \
        "json.num_bikes_available", \
        "json.is_installed", \
        "json.is_renting", \
        "json.is_returning", \
        "json.last_reported" \
    ) \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_station_status") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesStationStatus) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_station_status") \
    .start()


    def upsertSharedBikesBikeStatus(microBatchOutputDF, batchId):
      microBatchOutputDF.createOrReplaceTempView("sharedBikesBikeStatusUpserts")

      microBatchOutputDF._jdf.sparkSession().sql("""
        MERGE INTO lakehouse.smart_city.shared_bikes_bike_status as t
        USING (SELECT DISTINCT bike_id, vehicle_type_id, latitude, longitude, is_reserved, is_disabled, last_reported FROM sharedBikesBikeStatusUpserts) as u
        ON u.bike_id = t.bike_id AND u.last_reported = t.last_reported
        WHEN NOT MATCHED THEN INSERT *
      """)

    schema = StructType([ \
        StructField("lat", DoubleType(), True), \
        StructField("lon", DoubleType(), True), \
        StructField("bike_id", StringType(), True), \
        StructField("is_reserved", BooleanType(), True), \
        StructField("is_disabled", BooleanType(), True), \
        StructField("vehicle_type_id", StringType(), True), \
        StructField("last_reported", TimestampType(), True), \
    ])
    spark \
    .readStream \
    .format("kafka") \
    .options(**kafkaOptions) \
    .option("subscribe", "shared_bikes_bike_status") \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load() \
    .selectExpr("cast(key as string)", "cast(value as string)") \
    .withColumn("json", from_json(col("value"), schema)) \
    .selectExpr("json.bike_id", "json.vehicle_type_id", "json.lat as latitude", "json.lon as longitude", "json.is_reserved", "json.is_disabled", "json.last_reported") \
    .writeStream \
    .queryName("ingest smart_city shared_bikes_bike_status") \
    .format("iceberg") \
    .foreachBatch(upsertSharedBikesBikeStatus) \
    .outputMode("update") \
    .trigger(processingTime='2 minutes') \
    .option("checkpointLocation", "s3a://lakehouse/smart-city/checkpoints/shared_bikes_bike_status") \
    .start()


    # key: table name
    # value: compaction strategy
    tables_to_compact = {
        "lakehouse.water_levels.stations": "",
        "lakehouse.water_levels.measurements": ", strategy => 'sort', sort_order => 'timestamp DESC NULLS LAST,station_uuid ASC NULLS LAST'",
        "lakehouse.smart_city.shared_bikes_station_information": "",
        "lakehouse.smart_city.shared_bikes_station_status": ", strategy => 'sort', sort_order => 'last_reported DESC NULLS LAST,station_id ASC NULLS LAST'",
        "lakehouse.smart_city.shared_bikes_bike_status": "",
    }

    while True:
        expire_before = (datetime.now() - timedelta(hours=12)).strftime("%Y-%m-%d %H:%M:%S")
        for table, table_compaction_strategy in tables_to_compact.items():
            print(f"[{table}] Expiring snapshots older than 12 hours ({expire_before})")
            spark.sql(f"CALL lakehouse.system.expire_snapshots(table => '{table}', older_than => TIMESTAMP '{expire_before}', retain_last => 50, stream_results => true)")

            print(f"[{table}] Removing orphaned files")
            spark.sql(f"CALL lakehouse.system.remove_orphan_files(table => '{table}')")

            print(f"[{table}] Starting compaction")
            spark.sql(f"CALL lakehouse.system.rewrite_data_files(table => '{table}'{table_compaction_strategy})")
            print(f"[{table}] Finished compaction")

        print("All tables compacted. Waiting 25min before scheduling next run...")
        time.sleep(25 * 60) # Assuming compaction takes 5 min run every 30 minutes
