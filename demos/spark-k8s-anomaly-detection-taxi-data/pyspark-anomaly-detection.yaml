---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-spark
data:
  spark-ad.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import dayofweek, to_date, date_format, year, hour, minute, month, when, dayofmonth, dayofweek
    from pyspark.sql.functions import concat_ws, substring, concat, lpad, lit
    from pyspark.sql.functions import round, sum, count, avg
    from pyspark.sql.functions import lag
    from pyspark.sql.window import Window
    from pyspark.sql import functions, types
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    
    spark = SparkSession.builder.appName("NY TLC AD").getOrCreate()
    input_df = spark.read.parquet("s3a://demo/ny-taxi-data/raw/")
    
    df = input_df.select(
        to_date(input_df.tpep_pickup_datetime).alias("day_date")
        , year(input_df.tpep_pickup_datetime).alias('year')
        , month(input_df.tpep_pickup_datetime).alias('month')
        , dayofmonth(input_df.tpep_pickup_datetime).alias("dayofmonth")
        , dayofweek(input_df.tpep_pickup_datetime).alias("dayofweek")
        , hour(input_df.tpep_pickup_datetime).alias("hour")
        , minute(input_df.tpep_pickup_datetime).alias("minute")
        , input_df.total_amount.alias("driver_pay")
    )
  
    df = df.withColumn("minute_group", when(df.minute < 30, '00').otherwise('30'))
    df = df.withColumn("time_group",concat_ws(":", lpad(df.hour, 2, '0'), df.minute_group, lit('00')))
    df = df.withColumn("ts",concat_ws(" ", df.day_date, df.time_group))
    
    dfs = df.select(
        date_format(df.ts, "yyyy-MM-dd HH:mm:ss").alias("date_group")
        , df.year
        , df.hour
        , df.month
        , df.dayofmonth
        , df.dayofweek
        , df.driver_pay
    ).groupby("date_group", "hour", "year", "month", "dayofmonth", "dayofweek").agg(functions.count('driver_pay').alias('no_rides'), functions.round(functions.sum('driver_pay'), 2).alias('total_bill'), functions.round(functions.avg('driver_pay'), 2).alias('avg_bill')).orderBy("date_group")
    
    dfs.show()
    
    windowSpec  = Window.partitionBy("hour").orderBy("date_group")
    
    dfs = dfs.withColumn("lag",lag("no_rides",2).over(windowSpec))
    dfs = dfs.filter("lag IS NOT NULL")
    dfs.show()
    
    scaler = StandardScaler()
    classifier = IsolationForest(contamination=0.005, n_estimators=200, max_samples=0.7, random_state=42, n_jobs=-1)
    
    df_model = dfs.select(dfs.hour, dfs.year, dfs.month, dfs.dayofmonth, dfs.dayofweek, dfs.no_rides, dfs.total_bill, dfs.avg_bill, dfs.lag)
    
    x_train = scaler.fit_transform(df_model.collect())
    clf = classifier.fit(x_train)
    
    SCL = spark.sparkContext.broadcast(scaler)
    CLF = spark.sparkContext.broadcast(clf)
  
    def predict_using_broadcasts(hour, year, month, dayofmonth, dayofweek, no_rides, total_bill, avg_bill, lag):
      prediction = 0
      x_test = [[hour, year, month, dayofmonth, dayofweek, no_rides, total_bill, avg_bill, lag]]
      try:
        x_test = SCL.value.transform(x_test)
        prediction = CLF.value.predict(x_test)[0]
      except ValueError:
        import traceback
        traceback.print_exc()
        print('Cannot predict:', x_test)
      return int(prediction)
    
    udf_predict_using_broadcasts = functions.udf(predict_using_broadcasts, types.IntegerType())
    
    df_pred = df_model.withColumn(
        'prediction',
        udf_predict_using_broadcasts('hour', 'year', 'month', 'dayofmonth', 'dayofweek', 'no_rides', 'total_bill', 'avg_bill', 'lag')
    )
    df_pred.show()
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: spark-ad
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.3.0-stackable0.2.0
  mode: cluster
  mainApplicationFile: local:///spark-scripts/spark-ad.py
  deps:
    requirements:
      - scikit-learn==0.24.2
  volumes:
    - name: cm-spark
      configMap:
        name: cm-spark
  driver:
    volumeMounts:
      - name: cm-spark
        mountPath: /spark-scripts
  executor:
    volumeMounts:
      - name: cm-spark
        mountPath: /spark-scripts
  sparkConf:
    spark.kubernetes.submission.waitAppCompletion: "false"
    spark.kubernetes.driver.pod.name: "spark-ad-driver"
    spark.kubernetes.executor.podNamePrefix: "spark-ad"
    spark.kubernetes.driver.request.cores: "2"
    spark.kubernetes.driver.limit.cores: "3"
    spark.driver.cores: "3"
    spark.driver.memory: "1g"
    spark.driver.memoryOverheadFactor: "0.4"
    spark.kubernetes.executor.request.cores: "2"
    spark.kubernetes.executor.limit.cores: "3"
    spark.executor.cores: "3"
    spark.executor.memory: "4g"
    spark.executor.memoryOverheadFactor: "0.4"
    spark.executor.instances: "6"
    spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
  s3bucket:
    inline:
      bucketName: demo
      connection:
        inline:
          host: minio-trino
          port: 9000
          accessStyle: Path
          credentials:
            secretClass: s3-credentials-class
---
apiVersion: v1
kind: Secret
metadata:
  name: minio-credentials
  labels:
    secrets.stackable.tech/class: s3-credentials-class
stringData:
  accessKeyId: demo
  secretAccessKey: demodemo
---
apiVersion: secrets.stackable.tech/v1alpha1
kind: SecretClass
metadata:
  name: s3-credentials-class
spec:
  backend:
    k8sSearch:
      searchNamespace:
        pod: {}
