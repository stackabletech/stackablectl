---
apiVersion: batch/v1
kind: Job
metadata:
  name: create-hbase-row-key
spec:
  template:
    spec:
      containers:
        - name: create-hbase-row-key
          image: docker.stackable.tech/stackable/testing-tools:0.1.0-stackable0.1.0
          command: ["bash", "-c", "python -u /tmp/script/script.py"]
          volumeMounts:
            - name: script
              mountPath: /tmp/script
          args: ["$(INPUT_FILE_NAME)"]
      volumes:
        - name: script
          configMap:
            name: generate-hash-row-key
      restartPolicy: OnFailure
  backoffLimit: 50
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: generate-hash-row-key
data:
  script.py: |
  
      import argparse
      import hashlib
      import pandas as pd
    
    
    def parse_args():
      parser = argparse.ArgumentParser(
      description=""
      )
      
      parser.add_argument(
      "-i",
      "--input_file",
      type=str,
      help="Name of the CSV input file.",
      required=True
      )
      
      parser.add_argument(
      "-o",
      "--output_file",
      type=str,
      help="Name of the CSV output file.",
      required=True
      )
      
      return parser.parse_args()
    
    
    def main():
      args = parse_args()
      
      output_file_dir = args.output_file
      
      df = pd.read_csv(args.input_file)
      
      df['visitorid'].astype(str) + df['timestamp'].astype(str)
      
      df2 = df.assign(concat_col=lambda x: (x.visitorid).astype(str) + (x.timestamp).astype(str) )
      
      df2['hash_col'] = df2['concat_col'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())
      
      df3 = df2[ ['hash_col'] + [ col for col in df2.columns if col != 'hash_col' ]]
      
      df3.to_csv(output_file_dir,columns=['hash_col','timestamp','visitorid','event','itemid','transactionid'] , sep=';', encoding='utf-8', index=False, na_rep='N/A')
    
    if __name__ == "__main__":
      main()