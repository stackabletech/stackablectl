---
apiVersion: batch/v1
kind: Job
metadata:
  name: start-pyspark-job
spec:
  template:
    spec:
      containers:
        - name: start-pyspark-job
          image: docker.stackable.tech/stackable/tools:0.2.0-stackable0.3.0
          command: ["bash", "-c", "
          kubectl wait airflowdb/airflow --for jsonpath='{.status.condition}'=Ready --timeout 600s
          && kubectl rollout status --watch statefulset/airflow-webserver-default
          && kubectl rollout status --watch statefulset/airflow-worker-default
          && kubectl rollout status --watch statefulset/airflow-scheduler-default
          && curl -i -s --user airflow:airflow http://airflow-webserver-default:8080/api/v1/dags/sparkapp_dag
          && curl -i -s --user airflow:airflow -H 'Content-Type:application/json' -XPATCH http://airflow-webserver-default:8080/api/v1/dags/sparkapp_dag -d '{\"is_paused\": false}'
          && curl -i -s --user airflow:airflow -H 'Content-Type:application/json' -XPOST http://airflow-webserver-default:8080/api/v1/dags/sparkapp_dag/dagRuns -d '{}'
          "]
      restartPolicy: OnFailure
  backoffLimit: 20 # give some time for the Airflow cluster to be available
