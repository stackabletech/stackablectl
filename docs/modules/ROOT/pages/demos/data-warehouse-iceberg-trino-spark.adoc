= data-warehouse-iceberg-trino-spark

[WARNING]
====
This demos uses significant amount of resources. It will most likely not run on your workstation.
It was developed and tested on 9 nodes with 4 cores (8 threads), 20GB RAM and 30GB hdd disks.
Additionally persistent volumes with a total size of approx. 500GB will get created.
A smaller version of this demo might be created in the future.
====

[NOTE]
====
This guide assumes you already have the demo `data-warehouse-iceberg-trino-spark` installed.
If you don't have it installed please follow the xref:commands/demo.adoc#_install_demo[documentation on how to install a demo].
To put it simply you have to run `stackablectl demo install data-warehouse-iceberg-trino-spark`.
====

This demo will

* Install the required Stackable operators
* Spin up the following data products
** *Trino*: A fast distributed SQL query engine for big data analytics that helps you explore your data universe. This demo uses it to enable SQL access to the data
** *Spark*: A multi-language engine for executing data engineering, data science, and machine learning. This demo uses it to stream data from Kafka into the warehouse
** *MinIO*: A S3 compatible object store. This demo uses it as persistent storage to store all the data used
** *Kafka*:  A distributed event streaming platform for high-performance data pipelines, streaming analytics and data integration. This demos uses it as an event streaming platform to stream the data in near real-time
** *NiFi*:  An easy-to-use, powerful system to process and distribute data. This demos uses it to fetch multiple online real-time datasources and ingest it into Kafka
** *Hive metastore*: A service that stores metadata related to Apache Hive and other services. This demo uses it as metadata storage for Trino and Spark
** *Open policy agent* (OPA): A open source, general-purpose policy engine that unifies policy enforcement across the stack. This demo uses it as the authorizer for Trino, which decides which user is able to query which data.
** *Superset*: A modern data exploration and visualization platform. This demo utilizes Superset to retrieve data from Trino via SQL queries and build dashboards on top of that data
* Copy multiple datasources in csv and parquet format into the s3 staging area
* Let Trino copy the data from staging area into the warehouse area. During the copy transformations such as validating, casting, parsing timestamps and enriching the data by joining lookup-tables are done
* Simultaneously start a NiFi workflow, which fetches real-time datasets via the internet and ingests the data as json records into Kafka
* Spark structured streaming job is started, which streams the data out of Kafka into the warehouse
* Create Superset dashboards for visualization of the different datasets

You can see the deployed products as well as their relationship in the following diagram:

image::demo-data-warehouse-iceberg-trino-spark/overview.png[]
