= data-lakehouse-iceberg-trino-spark

[WARNING]
====
This demos uses significant amount of resources. It will most likely not run on your workstation.
It was developed and tested on 10 nodes with each 4 cores (8 threads), 20GB RAM and 30GB HDD.
Additionally, persistent volumes with a total size of approximately 1TB will get created.
A smaller version of this demo might be created in the future.
====

[NOTE]
====
This guide assumes that you already have the demo `data-lakehouse-iceberg-trino-spark` installed.
If you don't have it installed please follow the xref:commands/demo.adoc#_install_demo[documentation on how to install a demo].
To put it simply you have to run `stackablectl demo install data-lakehouse-iceberg-trino-spark`.
====

This demo will

* Install the required Stackable operators
* Spin up the following data products
** *Trino*: A fast distributed SQL query engine for big data analytics that helps you explore your data universe. This demo uses it to enable SQL access to the data
** *Spark*: A multi-language engine for executing data engineering, data science, and machine learning. This demo uses it to stream data from Kafka into the lakehouse
** *MinIO*: A S3 compatible object store. This demo uses it as persistent storage to store all the data used
** *Kafka*: A distributed event streaming platform for high-performance data pipelines, streaming analytics and data integration. This demos uses it as an event streaming platform to stream the data in near real-time
** *NiFi*: An easy-to-use, powerful system to process and distribute data. This demos uses it to fetch multiple online real-time data sources and ingest it into Kafka
** *Hive metastore*: A service that stores metadata related to Apache Hive and other services. This demo uses it as metadata storage for Trino and Spark
** *Open policy agent* (OPA): An open source, general-purpose policy engine that unifies policy enforcement across the stack. This demo uses it as the authorizer for Trino, which decides which user is able to query which data.
** *Superset*: A modern data exploration and visualization platform. This demo utilizes Superset to retrieve data from Trino via SQL queries and build dashboards on top of that data
* Copy multiple data sources in CSV and Parquet format into the S3 staging area
* Let Trino copy the data from staging area into the lakehouse area. During the copy transformations such as validating, casting, parsing timestamps and enriching the data by joining lookup-tables are done
* Simultaneously start a NiFi workflow, which fetches datasets in real-time via the internet and ingests the data as JSON records into Kafka
* Spark structured streaming job is started, which streams the data out of Kafka into the lakehouse
* Create Superset dashboards for visualization of the different datasets

You can see the deployed products as well as their relationship in the following diagram:

image::demo-data-lakehouse-iceberg-trino-spark/overview.png[]

== Apache iceberg
As Apache iceberg states on their https://iceberg.apache.org/docs/latest/[website]:

> Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.

This demos uses Iceberg as it plays along nicely with object storage as well as having a Trino and Spark integration.

== List deployed Stackable services
To list the installed installed Stackable services run the following command:

[source,console]
----
$ stackablectl services list --all-namespaces
 PRODUCT    NAME          NAMESPACE  ENDPOINTS                                          EXTRA INFOS                             
                                                                                                                                
 hive       hive          default    hive                212.227.224.138:31022                                                  
                                     metrics             212.227.224.138:30459                                                  
                                                                                                                                
 hive       hive-iceberg  default    hive                212.227.233.131:31511                                                  
                                     metrics             212.227.233.131:30003                                                  
                                                                                                                                
 kafka      kafka         default    metrics             217.160.118.190:32160                                                  
                                     kafka               217.160.118.190:31736                                           
                                                                                                                                
 nifi       nifi          default    https               https://217.160.120.117:31499  Admin user: admin, password: adminadmin 
                                                                                                                                
 opa        opa           default    http                http://217.160.222.211:31767                                           
                                                                                                                                
 superset   superset      default    external-superset   http://212.227.233.47:32393    Admin user: admin, password: adminadmin      
                                                                                                                                
 trino      trino         default    coordinator-metrics 212.227.224.138:30610                                                  
                                     coordinator-https   https://212.227.224.138:30876                                          
                                                                                                                                
 zookeeper  zookeeper     default    zk                  212.227.224.138:32321                                                  
                                                                                                                                
 minio      minio         default    http                http://217.160.222.211:32031   Third party service                     
                                     console-http        http://217.160.222.211:31429   Admin user: admin, password: adminadmin 
----

[NOTE]
====
When a product instance has not finished starting yet, the service will have no endpoint.
Starting all the product instances might take a considerable amount of time depending on your internet connectivity.
In case the product is not ready yet a warning might be shown.
====

== MinIO
=== List buckets
The S3 provided by MinIO is used as persistent storage to store all the data used.
Open the `minio` endpoint `console-http` retrieved by `stackablectl services list` in your browser (http://217.160.222.211:31429 in this case).

image::demo-data-lakehouse-iceberg-trino-spark/minio_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::demo-data-lakehouse-iceberg-trino-spark/minio_2.png[]

Here you can see the two buckets contained in the S3:

1. `staging`: The demo loads static datasets into this area. It is stored in different formats, such as CSV and Parquet. It does contain actual data tables as well as lookup tables.
2. `lakehouse`: This bucket is where the cleaned and/or aggregated data resides. The data is stored in the https://iceberg.apache.org/[Apache Iceberg] table format.

=== Inspect lakehouse
Click on the blue button `Browse` on the bucket `lakehouse`.

image::demo-data-lakehouse-iceberg-trino-spark/minio_3.png[]

You can see multiple folders (called prefixes in S3) - each containing a different dataset.

Click on the folders `house-sales` afterwards the folder starting with `house-sales-*` afterwards 'data'.

image::demo-data-lakehouse-iceberg-trino-spark/minio_4.png[]

As you can see the table `house-sales` is partitioned by day.
Go ahead and click on any folder.

image::demo-data-lakehouse-iceberg-trino-spark/minio_5.png[]

You can see that Trino has placed a single file here containing all the house sales of that particular year.

== NiFi

NiFi is used to fetch multiple datasources from the internet and ingest it into Kafka near-realtime.
Some data sources are statically downloaded (e.g. as CSV) and others are dynamically fetched via APIs such as REST APIs.
This includes the following data sources:

* https://www.pegelonline.wsv.de/webservice/guideRestapi[Water level measurements in Germany] (real-time)
* https://mobidata-bw.de/dataset/bikesh[Shared bikes in Germany] (real-time)
* https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads[House sales in UK] (static)
* https://www.usgs.gov/programs/earthquake-hazards/earthquakes[Registered earthquakes worldwide] (static)
* https://mobidata-bw.de/dataset/e-ladesaulen[E-charging stations in Germany] (static)
* https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page[NewYork taxi data] (static)

=== View ingestion jobs
You can have a look at the ingestion job running in NiFi by opening the given `nifi` endpoint `https` from your `stackablectl services list` command output (https://217.160.120.117:31499 in this case).
If you get a warning regarding the self-signed certificate generated by the xref:secret-operator::index.adoc[Secret Operator] (e.g. `Warning: Potential Security Risk Ahead`), you have to tell your browser to trust the website and continue.

image::demo-data-lakehouse-iceberg-trino-spark/nifi_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::demo-data-lakehouse-iceberg-trino-spark/nifi_2.png[]

As you can see, the NiFi workflow consists of lots of components.
You can zoom in by using your mouse and mouse wheel.
On the left side are two strands, that

1. Fetch the list of known water-level stations and ingest them into Kafka
2. Continuously run a loop fetching the measurements of the last 30 for every measuring station and ingesting the measurements into Kafka

On the right side are three strands, that

1. Fetch the current shared bike stations information
2. Fetch the current shared bike stations status
3. Fetch the current shared bike bike status

For details on the NiFi workflow ingesting water-level data please read on the xref:demos/nifi-kafka-druid-water-level-data.adoc#_nifi[nifi-kafka-druid-water-level-data documentation on NiFi].

== Spark

https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[Spark Structured Streaming] is used to stream data from Kafka into the lakehouse.

To have access to the Spark WebUI you need to run the following command to port-forward the Port 4040 to your local machine

[source,console]
----
kubectl port-forward $(kubectl get pod -o name | grep 'spark-ingest-into-lakehouse-.*-driver') 4040
----

Afterwards you can reach the Webinterface on http://localhost:4040.

image::demo-data-lakehouse-iceberg-trino-spark/spark_1.png[]

On the UI the last jobs are shown.
Each running Structured Streaming job creates lots of Spark jobs internally.

Click on the tab `Structured Streaming` to see the running streaming jobs.

image::demo-data-lakehouse-iceberg-trino-spark/spark_2.png[]

Five streaming jobs are currently running.
The job with the highest throughput is the `ingest water_level measurements` job.
Click on the `Run ID` highlighted in blue.

image::demo-data-lakehouse-iceberg-trino-spark/spark_3.png[]

== Trino
Trino is used to enable SQL access to the data.

=== View WebUI
Open up the the given `trino` endpoint `coordinator-https` from your `stackablectl services list` command output (https://212.227.224.138:30876 in this case).

image::demo-data-lakehouse-iceberg-trino-spark/trino_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::demo-data-lakehouse-iceberg-trino-spark/trino_2.png[]

=== Connect with DBeaver
https://dbeaver.io/[DBeaver] is free multi-platform database tool that can be used to connect to Trino.
Please have a look at the <TODO> trino-operator documentation on how to connect DBeaver to Trino.

image::demo-data-lakehouse-iceberg-trino-spark/dbeaver_1.png[]

image::demo-data-lakehouse-iceberg-trino-spark/dbeaver_2.png[]
You need to modify the setting `TLS` to `true`.
Additionally you need to add the setting `SSLVerification` and set it to `NONE`.

image::demo-data-lakehouse-iceberg-trino-spark/dbeaver_3.png[]

Here you can see all the available Trino catalogs.

* `staging`: The staging area containing raw data in various data formats such as CSV or Parquet
* `system`: Internal catalog to retrieve Trino internals
* `tpcds`: https://trino.io/docs/current/connector/tpcds.html[TPCDS connector] providing a set of schemas to support the http://www.tpc.org/tpcds/[TPC Benchmark™ DS]
* `tpch`: https://trino.io/docs/current/connector/tpch.html[TPCH connector] providing a set of schemas to support the http://www.tpc.org/tpcds/[TPC Benchmark™ DS]
* `lakehouse`: The lakehouse area containing the enriched and performant accessible data

== Superset
Superset provides the ability to execute SQL queries and build dashboards.
Open the `superset` endpoint `external-superset` in your browser (http://212.227.233.47:32393 in this case).

image::demo-data-lakehouse-iceberg-trino-spark/superset_1.png[]

Log in with the username `admin` and password `adminadmin`.

image::demo-data-lakehouse-iceberg-trino-spark/superset_2.png[]

=== View dashboard
The demo has created dashboards to visualize the different data sources.
To the dashboards click on the tab `Dashboards` at the top.

image::demo-data-lakehouse-iceberg-trino-spark/superset_3.png[]

Click on the dashboard called `House sales`.
It might take some time until the dashboards renders all the included charts.

image::demo-data-lakehouse-iceberg-trino-spark/superset_4.png[]

Another dashboard to look at is `Earthquakes`.

image::demo-data-lakehouse-iceberg-trino-spark/superset_5.png[]

Another dashboard to look at is `Taxi trips`.

image::demo-data-lakehouse-iceberg-trino-spark/superset_6.png[]

There are multiple other dashboards you can explore on you own.

=== View charts

The dashboards consists of multiple charts.
To list the charts click on the tab `Charts` at the top.

=== Execute arbitrary SQL statements
Within Superset you can not only create dashboards but also run arbitrary SQL statements.
On the top click on the tab `SQL Lab` -> `SQL Editor`.

image::demo-data-lakehouse-iceberg-trino-spark/superset_7.png[]

On the left select the database `Trino lakehouse`, the schema `house_sales` and set `See table schema` to `house_sales`.

image::demo-data-lakehouse-iceberg-trino-spark/superset_8.png[]

On the right textbox enter the desired SQL statement.
If you do not want to make one up, you can use the following:

[source,sql]
----
select city, sum(price) as sales
from house_sales
group by 1
order by 2 desc
----

image::demo-data-lakehouse-iceberg-trino-spark/superset_9.png[]
