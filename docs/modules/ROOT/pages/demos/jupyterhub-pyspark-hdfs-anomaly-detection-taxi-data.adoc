= jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data

This demo showcases the integration between https://jupyter.org[Jupyter] and https://hadoop.apache.org/[Apache Hadoop] deployed on the Stackable Data Platform (SDP) Kubernetes cluster. https://jupyterlab.readthedocs.io/en/stable/[JupyterLab] is deployed using the https://github.com/jupyterhub/zero-to-jupyterhub-k8s[pyspark-notebook stack] provided by the Jupyter community. The SDP makes this integration easy by publishing a discovery `ConfigMap` for the HDFS cluster. This `ConfigMap` is then mounted in all `Pods`` running https://spark.apache.org/docs/latest/api/python/getting_started/index.html[PySpark] notebooks so that these have access to HDFS data. For this demo, the HDFS cluster is provisioned with a small sample of the https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page[NYC taxi trip dataset] which is analyzed with a notebook that is provisioned automatically in the JupyterLab interface .

[NOTE]
====
This guide assumes that the demo `jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data` is already installed.
If it is not installed yet, please follow the xref:commands/demo.adoc#_install_demo[documentation on how to install a demo].
To put it simply you have to run:

[source,bash]
----
$ kubectl create namespace jupyterhub-demo && \
stackablectl --namespace jupyterhub-demo demo install jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data
----

====

== Overview

This demo will:

* Install the required Stackable Data Platform operators
* Spin up the following data products
** *JupyterHub*: A multi-user server for Jupyter notebooks
** *HDFS*: A distributed file system used to store the taxi dataset
* Download a sample of the NY taxi dataset into HDFS
* Install Jupyter notebook
* Train an anomaly detection model using PySpark on the data available in HDFS
* Perform some predictions and visualize anomalies

== HDFS

The Stackable Operator for Apache HDFS will spin up a HDFS cluster in order to store the taxi dataset in parquet format. This dataset will be read and processed via PySpark.

Before trying out the notebook example in Jupyter, check if the taxi data was loaded to HDFS successfully:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo exec -c namenode -it hdfs-namenode-default-0 -- /bin/bash -c "./bin/hdfs dfs -ls /ny-taxi-data/raw"
Found 1 items
-rw-r--r--   3 stackable supergroup  314689382 2022-11-23 15:01 /ny-taxi-data/raw/fhvhv_tripdata_2020-09.parquet
----

There should be one parquet file containing taxi trip data from September 2020.

== JupyterHub

Have a look at the available Pods before logging in (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

JupyterHub will create a Pod for each active user. In order to reach the JupyterHub web interface, create a port-forward:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo port-forward service/proxy-public 8080:http
----

Now access the JupyterHub web interface via:

----
http://localhost:8080
----

You should see the JupyterHub login page.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_login.png[]

Log in with username `demo` and password `demo`.

There should appear a new pod called `jupyter-demo` (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
jupyter-demo                                     1/1     Running     0          20m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

You should arrive at your workspace:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_workspace.png[]

Now you can click on the `notebooks` folder on the left and open the contained file and run it. Click on the double arrow to execute the Python scripts.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_run_notebook.png[]

== Visualization

After the script finished successfully, several plots should be displayed on the bottom. Both show the same data in 2D and 3D representation. The 3D plot should look like this:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_3d_isolation_forest.png[]

The anomaly detection uses the https://en.wikipedia.org/wiki/Isolation_forest[Isolation Forest] model. This model attempts to isolate each data point by continually partitioning the data. Data closely packed together will require more partitioning to separate everything. Outliers just need up to two partitions. So the number of partitions is inversely proportional to the amount of anomaly.

TODO: this needs more explanation
