= jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data

This demo shows how to deploy https://jupyter.org/hub[JupyterHub] to a Kubernetes cluster using its https://github.com/jupyterhub/zero-to-jupyterhub-k8s[Helm] chart and connect it to a Stackable HDFS cluster. The HDFS cluster will contain https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page[NYC taxi trip record] data that will be analyzed for anomalies via a Jupyter notebook and https://spark.apache.org/docs/latest/api/python/[PySpark].

[NOTE]
====
This guide assumes that the demo `jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data` is already installed.
If it is not installed yet, please follow the xref:commands/demo.adoc#_install_demo[documentation on how to install a demo].
To put it simply you have to run:

[source,bash]
----
$ kubectl create namespace jupyterhub-demo && \
stackablectl demo install --namespace jupyterhub-demo jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data
----
====

== Overview

This demo will:

* Install the required Stackable operators and Helm charts
* Spin up the following data products
** *JupyterHub*: A multi-user server for Jupyter notebooks
** *HDFS*: A distributed file system used to store the taxi dataset
* Copy the taxi data in parquet format into HDFS
* Provide a predefined notebook in JupyterHub for anomaly detection
* Utilize PySpark for required anomaly computations
* Visualize the anomalies in the Jupyter notebook

== List deployed Stackable services

To list the installed Stackable services run the following command:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo get services
NAMESPACE         NAME                         TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                                                       AGE
jupyterhub-demo   hdfs-datanode-default        ClusterIP      None            <none>           8082/TCP,9866/TCP,9864/TCP,9867/TCP                           14m
jupyterhub-demo   hdfs-datanode-default-0      NodePort       10.15.254.165   <none>           8082:31190/TCP,9866:32102/TCP,9864:30970/TCP,9867:31002/TCP   14m
jupyterhub-demo   hdfs-journalnode-default     ClusterIP      None            <none>           8081/TCP,8480/TCP,8481/TCP,8485/TCP                           14m
jupyterhub-demo   hdfs-journalnode-default-0   NodePort       10.15.245.232   <none>           8081:31657/TCP,8480:32028/TCP,8481:31966/TCP,8485:31403/TCP   14m
jupyterhub-demo   hdfs-namenode-default        ClusterIP      None            <none>           8183/TCP,9870/TCP,8020/TCP                                    14m
jupyterhub-demo   hdfs-namenode-default-0      NodePort       10.15.254.187   <none>           8183:31835/TCP,9870:30753/TCP,8020:31491/TCP                  14m
jupyterhub-demo   hdfs-namenode-default-1      NodePort       10.15.248.18    <none>           8183:31134/TCP,9870:32212/TCP,8020:32485/TCP                  13m
jupyterhub-demo   hub                          ClusterIP      10.15.253.187   <none>           8081/TCP                                                      7m12s
jupyterhub-demo   proxy-api                    ClusterIP      10.15.250.69    <none>           8001/TCP                                                      7m12s
jupyterhub-demo   proxy-public                 Nodeport       10.15.241.45    ***.***.***.***   80:31331/TCP                                                  7m12s
jupyterhub-demo   zookeeper                    NodePort       10.15.245.181   <none>           2282:30161/TCP                                                15m
jupyterhub-demo   zookeeper-server-default     ClusterIP      None            <none>           2282/TCP,9505/TCP                                             15m
----

[NOTE]
====
When a product instance has not finished starting yet, the service will have no endpoint.
Starting all the product instances might take a considerable amount of time depending on your internet connectivity. In case the product is not ready yet a warning might be shown.
====

== HDFS

The Stackable Operator for Apache HDFS will spin up a HDFS cluster in order to store the taxi dataset in parquet format. This dataset will be read and processed via PySpark.

Before trying out the notebook example in Jupyter, check if the taxi data was loaded to HDFS successfully:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo exec -c namenode -it hdfs-namenode-default-0 -- /bin/bash -c "./bin/hdfs dfs -ls /ny-taxi-data/raw"
Found 1 items
-rw-r--r--   3 stackable supergroup  314689382 2022-11-23 15:01 /ny-taxi-data/raw/fhvhv_tripdata_2020-09.parquet
----

There should be one parquet file containing taxi trip data from September 2020.

== JupyterHub

Have a look at the available Pods before logging in (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

JupyterHub will create a Pod for each active user. In order to reach the JupyterHub web interface, create a port-forward:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo port-forward service/proxy-public 8080:http
----

Now access the JupyterHub web interface via:

----
http://localhost:8080
----

You should see the JupyterHub login page.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_login.png[]

Log in with username `demo` and password `demo`.

There should appear a new pod called `jupyter-demo` (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
jupyter-demo                                     1/1     Running     0          20m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

You should arrive at your workspace:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_workspace.png[]

Now you can click on the `notebooks` folder on the left and open the contained file and run it. Click on the double arrow to execute the Python scripts.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_run_notebook.png[]

== Visualization

After the script finished successfully, several plots should be displayed on the bottom. Both show the same data in 2D and 3D representation. The 3D plot should look like this:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_3d_isolation_forest.png[]

The anomaly detection uses the https://en.wikipedia.org/wiki/Isolation_forest[Isolation Forest] model. This model attempts to isolate each data point by continually partitioning the data. Data closely packed together will require more partitioning to separate everything. Outliers just need up to two partitions. So the number of partitions is inversely proportional to the amount of anomaly.

TODO: this needs more explanation
