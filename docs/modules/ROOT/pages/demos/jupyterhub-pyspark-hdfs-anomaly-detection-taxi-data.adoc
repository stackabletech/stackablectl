= jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data

This demo showcases the integration between https://jupyter.org[Jupyter] and https://hadoop.apache.org/[Apache Hadoop] deployed on the Stackable Data Platform (SDP) Kubernetes cluster. https://jupyterlab.readthedocs.io/en/stable/[JupyterLab] is deployed using the https://github.com/jupyterhub/zero-to-jupyterhub-k8s[pyspark-notebook stack] provided by the Jupyter community. The SDP makes this integration easy by publishing a discovery `ConfigMap` for the HDFS cluster. This `ConfigMap` is then mounted in all `Pods`` running https://spark.apache.org/docs/latest/api/python/getting_started/index.html[PySpark] notebooks so that these have access to HDFS data. For this demo, the HDFS cluster is provisioned with a small sample of the https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page[NYC taxi trip dataset] which is analyzed with a notebook that is provisioned automatically in the JupyterLab interface .

[NOTE]
====
This guide assumes that the demo `jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data` is already installed.
If it is not installed yet, please follow the xref:commands/demo.adoc#_install_demo[documentation on how to install a demo].
To put it simply you have to run:

[source,bash]
----
$ kubectl create namespace jupyterhub-demo && \
stackablectl --namespace jupyterhub-demo demo install jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data
----

====

== Overview

This demo will:

* Install the required Stackable Data Platform operators
* Spin up the following data products
** *JupyterHub*: A multi-user server for Jupyter notebooks
** *HDFS*: A distributed file system used to store the taxi dataset
* Download a sample of the NY taxi dataset into HDFS
* Install Jupyter notebook
* Train an anomaly detection model using PySpark on the data available in HDFS
* Perform some predictions and visualize anomalies

== HDFS

The Stackable Operator for Apache HDFS will spin up a HDFS cluster in order to store the taxi dataset in parquet format. This dataset will be read and processed via PySpark.

Before trying out the notebook example in Jupyter, check if the taxi data was loaded to HDFS successfully:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo exec -c namenode -it hdfs-namenode-default-0 -- /bin/bash -c "./bin/hdfs dfs -ls /ny-taxi-data/raw"
Found 1 items
-rw-r--r--   3 stackable supergroup  314689382 2022-11-23 15:01 /ny-taxi-data/raw/fhvhv_tripdata_2020-09.parquet
----

There should be one parquet file containing taxi trip data from September 2020.

== JupyterHub

Have a look at the available Pods before logging in (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

JupyterHub will create a Pod for each active user. In order to reach the JupyterHub web interface, create a port-forward:

[source,bash]
----
$ kubectl --namespace jupyterhub-demo port-forward service/proxy-public 8080:http
----

Now access the JupyterHub web interface via:

----
http://localhost:8080
----

You should see the JupyterHub login page.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_login.png[]

Log in with username `demo` and password `demo`.

There should appear a new pod called `jupyter-demo` (operator pods are left out for clarity, you will see more Pods):

[source,bash]
----
$ kubectl get pods --namespace jupyterhub-demo
NAME                                             READY   STATUS      RESTARTS   AGE
continuous-image-puller-87dzk                    1/1     Running     0          29m
continuous-image-puller-8qq7m                    1/1     Running     0          29m
continuous-image-puller-9xbss                    1/1     Running     0          29m
hdfs-datanode-default-0                          1/1     Running     0          29m
hdfs-journalnode-default-0                       1/1     Running     0          29m
hdfs-namenode-default-0                          2/2     Running     0          29m
hdfs-namenode-default-1                          2/2     Running     0          28m
hub-66c6798b9c-q877t                             1/1     Running     0          29m
jupyter-demo                                     1/1     Running     0          20m
load-test-data-wsqpk                             0/1     Completed   0          25m
proxy-65955f56cf-tf4ns                           1/1     Running     0          29m
user-scheduler-8d888c6d4-jb4mm                   1/1     Running     0          29m
user-scheduler-8d888c6d4-qbqkq                   1/1     Running     0          29m
----

You should arrive at your workspace:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_workspace.png[]

Now you can click on the `notebooks` folder on the left and open the contained file and run it. Click on the double arrow to execute the Python scripts. You can inspect the `hdfs` folder as well where the `core-site.xml` and `hdfs-site.xml` from the discovery `ConfigMap` of the HDFS cluster are located.

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_run_notebook.png[]

== Model details

The job uses an implementation of the Isolation Forest https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf[algorithm] provided by the scikit-learn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html[library]: the model is trained in a single task, but is then distributed to each executor from where it is invoked by a user-defined function (see https://towardsdatascience.com/isolation-forest-and-spark-b88ade6c63ff[this article] for how to call the sklearn library with a pyspark UDF). This type of model attempts to isolate each data point by continually partitioning the data. Data closely packed together will require more partitions to separate data points, whereas any outliers will require less: the number of required partitions for a particular data point is thus inversely proportional to the anomaly "score".

== Visualization

The notebook shows how to plot the outliers against a particular metric (e.g. "number of rides"). However, this is mainly for convenience - the anomaly score is derived from the *_entire_* feature space i.e. it takes all dimensions (or features/columns) into account when scoring data. This means that not only are the results difficult to visualize (how can multidimensional data be represented in only 3-D dimensional space?), but that a root cause analysis has to be a separate process. It would be tempting to look at just one metric and assume causal affects, but the model "sees" all features as a set of numerical values and derives patterns accordingly.

We can tackle the first of these issues by collapsing - or projecting - our data down into a manageable number of dimensions that can be plotted: once the script has finished successfully, plots should be displayed on the bottom that show the same data in 2D and 3D representation. The 3D plot should look like this:

image::demo-jupyterhub-pyspark-hdfs-anomaly-detection-taxi-data/jupyter_hub_3d_isolation_forest.png[]

It is clear that the model has detected outliers even though that would not have been immediately apparent from the time-series representation alone.

